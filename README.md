
# ğŸ“„ README.md

````markdown
# ğŸ¤– Content Agent â€“ Multi-Agent AI Backend

A scalable FastAPI-based multi-agent system built using LangChain and LLM pipelines.  
This project processes user queries through structured agent workflows and returns refined responses.

---

## ğŸš€ Features

- âš¡ FastAPI async backend
- ğŸ§  Multi-agent pipeline architecture
- ğŸ” Review & refinement agent
- ğŸŒ SERP API integration (if enabled)
- ğŸ” Environment-based configuration
- ğŸ“¦ uv for dependency management
- ğŸ“˜ Auto-generated Swagger docs

---

## ğŸ— Architecture

User Query  
â†“  
Primary Agent  
â†“  
Review Agent (Refactor & Validate Output)  
â†“  
Final Response  

The system is modular and designed for scalability.

---

## ğŸ›  Tech Stack

- Python 3.11+
- FastAPI
- LangChain
- Pydantic
- Uvicorn
- uv (dependency manager)

---

## ğŸ“¦ Installation

### 1ï¸âƒ£ Clone the repository

```bash
git clone https://github.com/your-username/your-repo.git
cd your-repo
```

### 2ï¸âƒ£ Install dependencies (using uv)

```bash
uv sync
```

---

## ğŸ” Environment Variables

Create a `.env` file in the project root:

```
OPENAI_API_KEY=your_openai_key
SERPAPI_KEY=your_serpapi_key
```

âš ï¸ Do NOT commit `.env`
Use `.env.example` for reference.

---

## â–¶ï¸ Run the Application

```bash
uv run main.py
```

Server will start at:

```
http://127.0.0.1:8000
```

Swagger Docs:

```
http://127.0.0.1:8000/docs
```

---

## ğŸ“¡ API Endpoint

### POST `/generate`

#### Request Body

```json
{
  "query": "What is 2 + 2?"
}
```

#### Response

```json
{
  "success": true,
  "response": "4"
}
```

---

